{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df852d25-b295-492a-a7fb-a782f958ac19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_ENABLE_MPS_FALLBACK=1\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90104171-1458-4e66-bcd9-671ce4f4eaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import hyclib as lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "868172f3-69e7-49db-a803-78c95936bc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "lib.logging.basic_config()\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aca5e8f2-6c3f-4c22-9fc2-6329717f90ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_tuple(out):\n",
    "    return out if isinstance(out, tuple) else (out,)\n",
    "\n",
    "def test_unique(M, shape, O, dim, sorted, return_index, return_inverse, return_counts, device):\n",
    "    kwargs = {\n",
    "        'return_index': return_index,\n",
    "        'return_inverse': return_inverse,\n",
    "        'return_counts': return_counts,\n",
    "    }\n",
    "    np_kwargs = kwargs.copy()\n",
    "    np_kwargs['return_index'] = True # always use return_index=True in order to have stable sort\n",
    "    \n",
    "    t = torch.randint(M, size=shape)\n",
    "    if O > 0:\n",
    "        t = t.float()\n",
    "        indices = [torch.randint(D, size=(O,)) for D in shape]\n",
    "        t[tuple(indices)] = torch.nan\n",
    "    t = t.to(device)\n",
    "    a = t.cpu().numpy()\n",
    "\n",
    "    torch_results = [t for t in as_tuple(lib.pt.unique(t, dim=dim, sorted=sorted, **kwargs))]\n",
    "    np_results = as_tuple(np.unique(t.cpu().numpy(), axis=dim, equal_nan=False, **np_kwargs))\n",
    "    if not kwargs['return_index']:\n",
    "        np_results = list(np_results)\n",
    "        del np_results[1]\n",
    "\n",
    "    if not sorted:\n",
    "        if dim is None:\n",
    "            sort_idx = torch_results[0].argsort(stable=True)\n",
    "        else:\n",
    "            sort_idx = lib.pt.lexsort(torch_results[0].movedim(dim, -1).flip(0))\n",
    "        sort_idx_inv = lib.pt.inv_perm(sort_idx)\n",
    "\n",
    "    keys = ['x'] + [k for k, v in kwargs.items() if v]\n",
    "\n",
    "    for key, torch_result, np_result in zip(keys, torch_results, np_results):\n",
    "        print(key, torch_result)\n",
    "        if sorted:\n",
    "            torch.testing.assert_close(torch_result, torch.from_numpy(np_result).to(device), equal_nan=True)\n",
    "        else:\n",
    "            if key == 'return_inverse':\n",
    "                torch.testing.assert_close(sort_idx_inv[torch_result], torch.from_numpy(np_result).to(device), equal_nan=True)\n",
    "            else:\n",
    "                if dim is None or key != 'x':\n",
    "                    torch.testing.assert_close(torch_result[sort_idx], torch.from_numpy(np_result).to(device), equal_nan=True)\n",
    "                else:\n",
    "                    torch.testing.assert_close(torch_result.movedim(dim, 0)[sort_idx].movedim(0, dim), torch.from_numpy(np_result).to(device), equal_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4736345d-4c1c-496a-9cbb-1e8b0c1fd2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor([[0., 0., 1., 2., 2., nan, 0., 2., nan],\n",
      "        [0., 2., 2., 1., 2., 2., nan, nan, 0.]])\n",
      "return_inverse tensor([5, 0, 1, 4, 6, 3, 0, 2, 7, 8])\n"
     ]
    }
   ],
   "source": [
    "M, shape, O, dim = (3, (2, 10), 5, 1)\n",
    "sorted = False\n",
    "return_index = False\n",
    "return_inverse = True\n",
    "return_counts = False\n",
    "device = 'cpu'\n",
    "test_unique(M, shape, O, dim, sorted, return_index, return_inverse, return_counts, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e3daad4a-f1ed-46f3-b84f-da8731355bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59400\n",
      "21560\n"
     ]
    }
   ],
   "source": [
    "M, D, shape, O, dim = (4, 6, (99,100), 60_000, -1)\n",
    "t = torch.randint(M, size=(D,*shape))\n",
    "if O > 0:\n",
    "    t = t.float()\n",
    "    indices = [torch.randint(D, size=(O,))]\n",
    "    indices += [torch.randint(N, size=(O,)) for N in shape]\n",
    "    t[tuple(indices)] = torch.nan\n",
    "t = t.to(device)\n",
    "a = t.cpu().numpy()\n",
    "\n",
    "pt_idx = lib.pt.lexsort(t, dim=dim)\n",
    "np_idx = np.lexsort(a, axis=dim)\n",
    "\n",
    "print(t.numel())\n",
    "print(t[~t.isnan()].numel())\n",
    "\n",
    "torch.testing.assert_close(pt_idx, torch.from_numpy(np_idx).to(device), equal_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "663ff2ef-fe8b-442b-9dda-46140de62d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M, N, D, O = 2, 100_000, 20, 1_000\n",
    "# M, N, D, O = 3, 100, 2, 10\n",
    "M, N, D, O = 5, 10_000, 5, 1_000\n",
    "t = torch.randint(M, size=(N,D)).float()\n",
    "idx_0 = torch.randint(N, size=(O,))\n",
    "idx_1 = torch.randint(D, size=(O,))\n",
    "t[idx_0, idx_1] = torch.nan\n",
    "t = t.to(device)\n",
    "t_cpu = t.cpu()\n",
    "a = t.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fb001d39-cc8e-470f-aac6-0ce19f6e9463",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 0\n",
    "kwargs = {\n",
    "    'return_index': True,\n",
    "    'return_inverse': True,\n",
    "    'return_counts': True,\n",
    "}\n",
    "\n",
    "torch_results = [t for t in as_tuple(lib.pt.unique(t, dim=dim, **kwargs))]\n",
    "np_results = as_tuple(np.unique(t.cpu().numpy(), axis=dim, equal_nan=False, **kwargs))\n",
    "\n",
    "for torch_result, np_result in zip(torch_results, np_results):\n",
    "    torch.testing.assert_close(torch_result, torch.from_numpy(np_result).to(device), equal_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06a09c49-84e9-453c-a7d4-b89241451a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3902\n"
     ]
    }
   ],
   "source": [
    "dim = 0\n",
    "kwargs = {\n",
    "    'return_index': True,\n",
    "    'return_inverse': True,\n",
    "    'return_counts': True,\n",
    "}\n",
    "\n",
    "def as_tuple(out):\n",
    "    return out if isinstance(out, tuple) else (out,)\n",
    "\n",
    "torch_results = [t for t in as_tuple(lib.pt.unique(t, dim=dim, **kwargs))]\n",
    "np_results = as_tuple(np.unique(t.cpu().numpy(), axis=dim, equal_nan=False, **kwargs))\n",
    "print(len(torch_results[0]))\n",
    "\n",
    "sort_idx = lib.pt.lexsort(torch_results[0].t().flip(0))\n",
    "sort_idx_inv = lib.pt.inv_perm(sort_idx)\n",
    "\n",
    "keys = ['x'] + [k for k, v in kwargs.items() if v]\n",
    "\n",
    "for key, torch_result, np_result in zip(keys, torch_results, np_results):\n",
    "    if key == 'return_inverse':\n",
    "        torch.testing.assert_close(sort_idx_inv[torch_result], torch.from_numpy(np_result).to(device), equal_nan=True)\n",
    "    else:\n",
    "        torch.testing.assert_close(torch_result[sort_idx], torch.from_numpy(np_result).to(device), equal_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "575a336c-ac45-4037-8994-0adac1db793a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 0.098428 s\n",
       "File: /Users/hoyinchau/local_documents/research/hyclib/hyclib/pt/core.py\n",
       "Function: _unique_sorted at line 30\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    30                                           def _unique_sorted(x, dim=None, return_index=False, return_inverse=False, return_counts=False):\n",
       "    31         1          0.0      0.0      0.0      if dim is None:\n",
       "    32                                                   x = x.reshape(-1)\n",
       "    33                                                   dim = 0\n",
       "    34                                                   \n",
       "    35                                               # reshape to a 2D tensor where we compute unique rows\n",
       "    36         1     100000.0 100000.0      0.1      x = x.movedim(dim, 0)\n",
       "    37         1       3000.0   3000.0      0.0      shape = x.shape\n",
       "    38         1      20000.0  20000.0      0.0      x = x.reshape(shape[0], -1)\n",
       "    39                                               \n",
       "    40         1   59399000.0 59399000.0     60.3      sort_idx = lexsort(x.t().flip(0))\n",
       "    41         1     918000.0 918000.0      0.9      x = x[sort_idx]\n",
       "    42                                               \n",
       "    43         1       1000.0   1000.0      0.0      if return_index:\n",
       "    44         1   36436000.0 36436000.0     37.0          x, inverse, counts = torch.unique_consecutive(x, dim=0, return_inverse=True, return_counts=True)\n",
       "    45         1     517000.0 517000.0      0.5          tot_counts = torch.cat((counts.new_zeros(1), counts.int().cumsum(dim=0).long()))[:-1] # cast to int32 due to MPS limitation\n",
       "    46         1     386000.0 386000.0      0.4          index = torch.arange(len(inverse), device=tot_counts.device)[tot_counts]\n",
       "    47                                                   \n",
       "    48         1       1000.0   1000.0      0.0          out = {'x': x, 'index': index}\n",
       "    49         1       1000.0   1000.0      0.0          if return_inverse:\n",
       "    50         1       1000.0   1000.0      0.0              out['inverse'] = inverse\n",
       "    51         1          0.0      0.0      0.0          if return_counts:\n",
       "    52         1       1000.0   1000.0      0.0              out['counts'] = counts\n",
       "    53                                               else:\n",
       "    54                                                   ret = torch.unique_consecutive(x, dim=0, return_inverse=return_inverse, return_counts=return_counts)\n",
       "    55                                                   ret = list(ret) if isinstance(ret, tuple) else [ret]\n",
       "    56                                                   \n",
       "    57                                                   out = {'x': ret.pop(0)}\n",
       "    58                                                   if return_inverse:\n",
       "    59                                                       out['inverse'] = ret.pop(0)\n",
       "    60                                                   if return_counts:\n",
       "    61                                                       out['counts'] = ret.pop(0)\n",
       "    62                                                       \n",
       "    63                                               # correct for the fact that we reshaped and sorted before computing unqiue\n",
       "    64         1      32000.0  32000.0      0.0      out['x'] = out['x'].reshape(-1, *shape[1:]).movedim(0, dim)\n",
       "    65         1          0.0      0.0      0.0      if return_index or return_inverse:\n",
       "    66         1       1000.0   1000.0      0.0          if return_index:\n",
       "    67         1     211000.0 211000.0      0.2              out['index'] = sort_idx[out['index']]\n",
       "    68         1          0.0      0.0      0.0          if return_inverse:\n",
       "    69         1     287000.0 287000.0      0.3              sort_idx_inv = inv_perm(sort_idx)\n",
       "    70         1     110000.0 110000.0      0.1              out['inverse'] = out['inverse'][sort_idx_inv]\n",
       "    71                                               \n",
       "    72         1       1000.0   1000.0      0.0      if len(out) == 1:\n",
       "    73                                                   return out['x']\n",
       "    74         1       2000.0   2000.0      0.0      return tuple(out.values())\n",
       "\n",
       "Total time: 0.098452 s\n",
       "File: /Users/hoyinchau/local_documents/research/hyclib/hyclib/pt/core.py\n",
       "Function: unique at line 76\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    76                                           def unique(x, dim=None, sorted=True, return_index=False, return_inverse=False, return_counts=False):\n",
       "    77         1       1000.0   1000.0      0.0      if sorted or x.device.type == 'mps': # unique_dim currently not implemented for MPS, so do _unique_sorted instead, which does not uses unique_consecutive instead of unique_dim\n",
       "    78         1   98451000.0 98451000.0    100.0          return _unique_sorted(x, dim=dim, return_index=return_index, return_inverse=return_inverse, return_counts=return_counts)\n",
       "    79                                               \n",
       "    80                                               if dim is None:\n",
       "    81                                                   x = x.reshape(-1)\n",
       "    82                                                   dim = 0\n",
       "    83                                               \n",
       "    84                                               # reshape to a 2D tensor where we compute unique rows\n",
       "    85                                               x = x.movedim(dim, 0)\n",
       "    86                                               shape = x.shape\n",
       "    87                                               x = x.reshape(shape[0], -1)\n",
       "    88                                               \n",
       "    89                                               # sort rows such that rows with nans comes after rows without nans.\n",
       "    90                                               # This avoids torch.unique bug when dim is not None and input has nan.\n",
       "    91                                               isnan = x.isnan().any(dim=1)\n",
       "    92                                               x = torch.cat((x[~isnan], x[isnan]), dim=0)\n",
       "    93                                           \n",
       "    94                                               if return_index:\n",
       "    95                                                   # code copied from https://github.com/pytorch/pytorch/issues/36748#issuecomment-1474368922\n",
       "    96                                                   x, inverse, counts = torch.unique(x, dim=0, return_inverse=True, return_counts=True)\n",
       "    97                                                   inv_sort_idx = inverse.argsort(stable=True)\n",
       "    98                                                   tot_counts = torch.cat((counts.new_zeros(1), counts.cumsum(dim=0)))[:-1]\n",
       "    99                                                   index = inv_sort_idx[tot_counts]\n",
       "   100                                                   \n",
       "   101                                                   out = {'x': x, 'index': index}\n",
       "   102                                                   if return_inverse:\n",
       "   103                                                       out['inverse'] = inverse\n",
       "   104                                                   if return_counts:\n",
       "   105                                                       out['counts'] = counts\n",
       "   106                                               else:\n",
       "   107                                                   ret = torch.unique(x, dim=0, return_inverse=return_inverse, return_counts=return_counts)\n",
       "   108                                                   ret = list(ret) if isinstance(ret, tuple) else [ret]\n",
       "   109                                                   out = {'x': ret.pop(0)}\n",
       "   110                                                   if return_inverse:\n",
       "   111                                                       out['inverse'] = ret.pop(0)\n",
       "   112                                                   if return_counts:\n",
       "   113                                                       out['counts'] = ret.pop(0)\n",
       "   114                                               \n",
       "   115                                               # correct for the fact that we reshaped and sorted before computing unqiue\n",
       "   116                                               out['x'] = out['x'].reshape(-1, *shape[1:]).movedim(0, dim)\n",
       "   117                                               if return_index or return_inverse:\n",
       "   118                                                   sort_idx = torch.cat(((~isnan).nonzero(), isnan.nonzero())).squeeze()\n",
       "   119                                                   if return_index:\n",
       "   120                                                       out['index'] = sort_idx[out['index']]\n",
       "   121                                                   if return_inverse:\n",
       "   122                                                       sort_idx_inv = inv_perm(sort_idx)\n",
       "   123                                                       out['inverse'] = out['inverse'][sort_idx_inv]\n",
       "   124                                               \n",
       "   125                                               if len(out) == 1:\n",
       "   126                                                   return out['x']\n",
       "   127                                               return tuple(out.values())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f lib.pt.unique -f lib.pt._unique_sorted lib.pt.unique(t, dim=dim, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3848096b-4a78-4bed-ad2d-11d2665debbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([95389, 20])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lib.pt.unique(t, dim=dim, **kwargs)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d9b076d-3106-4dba-b106-cafa5637d338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 ms ± 219 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "167 ms ± 1.34 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "313 ms ± 8.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit lib.pt.unique(t, dim=dim, **kwargs)\n",
    "%timeit lib.pt.unique(t_cpu, dim=dim, sorted=False, **kwargs)\n",
    "%timeit lib.pt.unique(t_cpu, dim=dim, sorted=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "771191d5-d2d7-40c0-8e84-f90a19ce8bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318 ms ± 9.39 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.unique(a, axis=dim, equal_nan=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0418b2e-cb95-4eed-8cc9-933db323e4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.6 ms ± 113 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "144 ms ± 300 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "150 ms ± 386 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "226 ms ± 5.65 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "111 ms ± 1.44 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit lib.pt.unique(t, dim=dim, **kwargs)\n",
    "%timeit lib.pt.unique(t_cpu, dim=dim, sorted=False, **kwargs)\n",
    "%timeit lib.pt.unique(t_cpu, dim=dim, sorted=True, **kwargs)\n",
    "%timeit np.unique(a, axis=dim, equal_nan=False, **kwargs)\n",
    "%timeit lib.np.unique_rows(a, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "981d53cf-32ad-4473-a013-95383cef3fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoyinchau/opt/anaconda3/envs/hyclib/lib/python3.9/site-packages/torch/masked/maskedtensor/core.py:299: UserWarning: max is not implemented in __torch_dispatch__ for MaskedTensor.\n",
      "If you would like this operator to be supported, please file an issue for a feature request at https://github.com/pytorch/maskedtensor/issues with a minimal reproducible code snippet.\n",
      "In the case that the semantics for the operator are not trivial, it would be appreciated to also include a proposal for the semantics.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "no implementation found for 'torch._ops.aten.max.default' on types that implement __torch_dispatch__: [<class 'torch.masked.maskedtensor.core.MaskedTensor'>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[175], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMaskedTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hyclib/lib/python3.9/site-packages/torch/masked/maskedtensor/core.py:274\u001b[0m, in \u001b[0;36mMaskedTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunction():\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mTypeError\u001b[0m: no implementation found for 'torch._ops.aten.max.default' on types that implement __torch_dispatch__: [<class 'torch.masked.maskedtensor.core.MaskedTensor'>]"
     ]
    }
   ],
   "source": [
    "torch.masked.MaskedTensor(torch.tensor([1,2,3]), torch.tensor([True, True, False])).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "503a275e-d1a4-4a3d-b939-0dd0e02032e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    t = torch.normal(mean=0, std=1, size=(10,), device='mps')\n",
    "    out_1 = lib.pt.stats.bin(t.cpu(), bins=3)[0]\n",
    "    out_2 = lib.pt.stats.bin(t, bins=3)[0].cpu()\n",
    "    out_3 = lib.sp.stats.bin(t.cpu().numpy(), bins=3)[0]\n",
    "    assert (out_1 == out_2.cpu()).all() and (out_1.numpy() == out_3).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fcd52dd0-03ed-43b8-a747-2b7eae192139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([3, 2, 3, 3, 3, 1, 3, 2, 2, 3]), tensor([    nan, -1.2486, -0.3541,  0.5404,     nan]), tensor([-1.6958, -0.8013,  0.0931,  0.9876]))\n",
      "(tensor([3, 2, 3, 3, 3, 1, 3, 2, 2, 3], device='mps:0'), tensor([    nan, -1.2486, -0.3541,  0.5404,     nan], device='mps:0'), tensor([-1.6958, -0.8013,  0.0931,  0.9876], device='mps:0'))\n",
      "(array([3, 2, 3, 3, 3, 1, 3, 2, 2, 3]), array([        nan, -1.24855185, -0.3540917 ,  0.54036844,         nan]), array([-1.695782  , -0.8013218 ,  0.09313837,  0.98759854], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "out_1 = lib.pt.stats.bin(a.cpu(), bins=3)\n",
    "out_2 = lib.pt.stats.bin(a, bins=3)\n",
    "out_3 = lib.sp.stats.bin(a.cpu(), bins=3)\n",
    "print(out_1)\n",
    "print(out_2)\n",
    "print(out_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c29e4d14-49e2-403e-a03d-22890f6af8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N = 100, 100000\n",
    "\n",
    "# bins = [-2.5,-2,-1,0,1,2,3.0]\n",
    "bins = 100\n",
    "arr = np.random.normal(size=N)\n",
    "indices = np.random.randint(N, size=M)\n",
    "arr[indices] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b9c6799-9588-4b70-8037-07463b5559da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array([-0.79857488, -0.29282404,  0.29925128, ..., -0.66033549,\n       -0.61356288,  0.04627834]) contains non-finite values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m binnumbers, centers, edges \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbin\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnan_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraise\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(arr)\n\u001b[1;32m      4\u001b[0m tbinnumbers, tcenters, tedges \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mpt\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mbin(t, bins\u001b[38;5;241m=\u001b[39mbins, nan_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/local_documents/research/hyclib/build/__editable__.hyclib-0.1.6-py3-none-any/hyclib/sp/stats.py:372\u001b[0m, in \u001b[0;36mbin\u001b[0;34m(sample, bins, range, nan_policy)\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mrange\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    370\u001b[0m         \u001b[38;5;28mrange\u001b[39m \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mrange\u001b[39m]\n\u001b[0;32m--> 372\u001b[0m bin_nums, centers, edges \u001b[38;5;241m=\u001b[39m \u001b[43mbin_dd\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnan_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bin_nums[\u001b[38;5;241m0\u001b[39m], centers[\u001b[38;5;241m0\u001b[39m], edges[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/local_documents/research/hyclib/build/__editable__.hyclib-0.1.6-py3-none-any/hyclib/sp/stats.py:332\u001b[0m, in \u001b[0;36mbin_dd\u001b[0;34m(sample, bins, range, expand_binnumbers, nan_policy)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# If bins was an integer-like object, now it is an actual Python int.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# NOTE: for _bin_edges(), see e.g. gh-11365\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nan_policy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(sample)\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m contains non-finite values.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sample,))\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# `Ndim` is the number of dimensions (e.g. `2` for `binned_statistic_2d`)\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# `Dlen` is the length of elements along each dimension.\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# This code is based on np.histogramdd\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# `sample` is an ND-array.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: array([-0.79857488, -0.29282404,  0.29925128, ..., -0.66033549,\n       -0.61356288,  0.04627834]) contains non-finite values."
     ]
    }
   ],
   "source": [
    "binnumbers, centers, edges = lib.sp.stats.bin(arr, bins=bins, nan_policy='raise')\n",
    "\n",
    "t = torch.tensor(arr)\n",
    "tbinnumbers, tcenters, tedges = lib.pt.stats.bin(t, bins=bins, nan_policy='raise')\n",
    "\n",
    "pbinnumbers, pedges = pd.cut(arr, bins=bins, retbins=True, labels=False, right=False)\n",
    "\n",
    "# print(binnumbers)\n",
    "# print(tbinnumbers.numpy())\n",
    "# print(pbinnumbers)\n",
    "# print(centers)\n",
    "# print(tcenters.numpy())\n",
    "# print(binnumbers[np.isnan(arr)])\n",
    "# print(tbinnumbers.numpy()[np.isnan(arr)])\n",
    "# print(pbinnumbers[np.isnan(arr)])\n",
    "\n",
    "torch.testing.assert_close(torch.from_numpy(binnumbers), tbinnumbers)\n",
    "torch.testing.assert_close(torch.from_numpy(centers), tcenters, equal_nan=True)\n",
    "torch.testing.assert_close(torch.from_numpy(edges), tedges)\n",
    "\n",
    "isnan = np.isnan(pbinnumbers)\n",
    "np.testing.assert_allclose(binnumbers[~isnan], pbinnumbers[~isnan] + 1)\n",
    "if isinstance(bins, numbers.Number):\n",
    "    r = np.nanmax(arr) - np.nanmin(arr)\n",
    "    edges[-1] = edges[-1] + r * 0.001\n",
    "np.testing.assert_allclose(edges, pedges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c302e7dc-bcbb-4351-bd9d-6f3212ebe81e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-hyclib_nightly]",
   "language": "python",
   "name": "conda-env-anaconda3-hyclib_nightly-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
